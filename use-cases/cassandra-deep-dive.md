# ğŸ“Š Apache Cassandra - Wide Column NoSQL Database

## ğŸ¯ Overview

Apache Cassandra is a **distributed, highly available, eventually consistent, wide-column NoSQL database** designed to handle huge amounts of structured data across many commodity servers.

### âœ¨ Key Characteristics
- ğŸ”„ **Peer-to-peer architecture** - No single point of failure
- ğŸ“ˆ **Linear scalability** - Performance increases linearly with added nodes
- âœï¸ **Write-optimized** - Exceptional write throughput
- ğŸŒ **Multi-datacenter replication** - Geographic distribution support
- ğŸš« **No master node** - Every node can serve client requests

---

## ğŸ—ï¸ Data Model

### Basic Building Blocks

```
ğŸ“¦ Column (Key-Value Pair)
   â”œâ”€â”€ Column Key: Unique identifier
   â””â”€â”€ Column Value: Stores one or collection of values

ğŸ“‹ Row: Collection of columns
ğŸ“ Table: Container of rows
ğŸ—„ï¸ Keyspace: Container for tables (spans one or more nodes)
ğŸŒ Cluster: Container of keyspaces
ğŸ’» Node: Computer system running Cassandra instance
```

### Hierarchy
```
Cluster
  â””â”€â”€ Keyspace(s)
       â””â”€â”€ Table(s)
            â””â”€â”€ Row(s)
                 â””â”€â”€ Column(s)
```

### ğŸ¯ Use Cases
Perfect for:
- â±ï¸ Time-series data (IoT sensor logs)
- ğŸ“Š Streaming services
- ğŸ’¬ Messaging applications
- ğŸ“ Activity logging
- ğŸŒ¡ï¸ Weather data
- ğŸ“ˆ Financial transactions

---

## ğŸ”‘ Partitioning & Data Distribution

### Partitioner

The **Partitioner** determines how data is distributed across the cluster using a **consistent hash ring**.

```
Request â†’ Partition Key â†’ Murmur3 Hash â†’ Token Ring â†’ Node Assignment
```

#### ğŸ” How Coordinator Finds Nodes

When a request arrives:
1. **Takes the partition key**
2. **Hashes it using Murmur3** (default hashing function)
3. **Maps hash to token ring**
4. **Builds preference list** (replica locations)

**Key Benefit**: Decouples client access from data ownership â†’ enables linear scalability without masters

---

## ğŸ” Replication

### Replication Factor (RF)
- **RF = 3** means each row stored on **3 different nodes**
- Each keyspace can have **different RF**

### ğŸ“‹ Replication Strategies

#### 1ï¸âƒ£ Simple Strategy
- For **single datacenter** deployments
- First replica placed by partitioner
- Subsequent replicas on **next nodes clockwise**

#### 2ï¸âƒ£ Network Topology Strategy (Production Default)
- For **multi-datacenter** deployments
- Different RF per datacenter
- Rack-aware placement

### ğŸ¯ Consistency Levels

```
Quorum = floor(RF/2 + 1)

Examples (RF = 3):
â”œâ”€â”€ ONE: 1 node must respond
â”œâ”€â”€ QUORUM: 2 nodes must respond  
â”œâ”€â”€ ALL: 3 nodes must respond
â”œâ”€â”€ LOCAL_QUORUM: Quorum in same DC
â””â”€â”€ EACH_QUORUM: Quorum in each DC
```

**Trade-off**: Higher consistency = Higher latency

---

## ğŸ“ Write Path (High Performance Secret)

### Write Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Write Requestâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CommitLog   â”‚    â”‚MemTable  â”‚
â”‚  (Disk)     â”‚    â”‚ (Memory) â”‚
â”‚  WAL/Append â”‚    â”‚  Sorted  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                        â”‚ (when full)
                        â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ SSTable  â”‚
                  â”‚  (Disk)  â”‚
                  â”‚Immutable â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
![Cassandra's write path](../images/cassandraWritePath.png)

### ğŸ”¥ Why Writes Are Fast
1. **Sequential append** to CommitLog (no seek time)
2. **In-memory writes** to MemTable (fast)
3. **Batch flushes** to SSTables (efficient)
4. **No read-before-write** (unlike B-trees)

### âš™ï¸ Components

#### CommitLog (Write-Ahead Log)
- ğŸ’¾ Durability guarantee
- ğŸ“Š Sequential writes (append-only)
- ğŸ”„ Replayed on node restart
- ğŸ“¦ **Segmented**: Multiple files, archived when data flushed

**Default**: 3 segments, each grows until threshold

#### MemTable
- ğŸ§  One per table per node (in-memory)
- ğŸ”€ Stores data **sorted by partition + clustering keys**
- âš¡ Serves reads for unflushed data
- ğŸ’§ Flushed when size threshold reached
![Storing data to commitlog and memtable](../images/storingDataToCommitLogAndMemTable.png)

**CommitLog vs MemTable**:
```
CommitLog: Sequential Order â†’ Fast writes
MemTable:  Sorted Order â†’ Fast reads
```

#### SSTable (Sorted String Table)
- ğŸ’½ Immutable on-disk files
- ğŸ”’ **Cannot be modified** after creation
- ğŸ—‘ï¸ Updates/Deletes = New write operations
- ğŸ“‘ Multiple SSTables per table
![Cassandra read operation workflow](../images/cassandraReadOperationWorkflow.png)

**Q: How to update/delete if immutable?**
**A**: Cassandra writes a new version with timestamp/tombstone

---

## ğŸ“– Read Path

### Read Flow
```
Read Request
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Row Cache  â”‚ (Hot rows)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ miss
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Key Cache  â”‚ (Partition key â†’ SSTable offset)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ miss
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bloom Filter   â”‚ (Key exists in SSTable?)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ probably yes
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition Index  â”‚
â”‚  Summary (RAM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition Index  â”‚
â”‚   File (Disk)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data File  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
![Anatomy of Cassandra write path](../images/anatomyOfCassandraWritePath.png)

### ğŸš€ Caching Layers

1. **Row Cache** ğŸ”¥
   - Caches entire frequently-read rows
   - Stored off-heap
   - Best for read-heavy workloads

2. **Key Cache** ğŸ—ï¸
   - Maps partition keys â†’ SSTable offsets
   - Stored on-heap
   - Updated on every write (can slow writes)

3. **Chunk Cache** ğŸ“¦
   - Uncompressed data chunks from SSTables
   - Frequently accessed data

![Anatomy of Cassandra read path](../images/anatomyOfCassandraReadPath.png)

### ğŸŒ¸ Bloom Filters
- **Probabilistic data structure**
- Tells if key **might exist** in SSTable
- **No false negatives** (if says "no", definitely not there)
- **Possible false positives** (if says "yes", might not be there)
- One per SSTable (stored in RAM)

**Benefit**: Avoids unnecessary disk reads

---

## ğŸ”§ Compaction

### ğŸ’¡ Why Compaction?

```
Problem: Multiple SSTables accumulate over time
         â†“
Solution: Merge them into fewer, larger SSTables
         â†“
Benefits: 
  - Faster reads (fewer files to check)
  - Remove deleted data (tombstones)
  - Reclaim disk space
```

### ğŸ“Š Compaction Strategies

#### 1ï¸âƒ£ Size-Tiered Compaction Strategy (STCS)
- Merges SSTables of **similar size**
- **Best for**: Write-heavy, time-series data
- **Drawback**: Can use 2x disk space temporarily

#### 2ï¸âƒ£ Leveled Compaction Strategy (LCS)
- Organizes SSTables into **levels** (L0, L1, L2...)
- Each level 10x larger than previous
- **Best for**: Read-heavy workloads
- **Benefit**: Predictable read performance

#### 3ï¸âƒ£ Time-Window Compaction Strategy (TWCS)
- Compacts within **time windows**
- **Best for**: Time-series with TTL
- **Example**: Compact hourly/daily buckets

---

## ğŸª¦ Tombstones (The Delete Challenge)

### ğŸ¤” The Problem
```
Node A: DELETE key=123
Node B: Offline (missed the delete)
         â†“
Node B comes back online
         â†“
Repair runs â†’ Node B resurrects deleted data! ğŸ˜±
```

### âœ… The Solution: Tombstones
- Delete = **Soft delete** (mark with tombstone)
- Tombstone = marker with **timestamp**
- Default TTL: **10 days** (gc_grace_seconds)
- Removed during compaction

### âš ï¸ Tombstone Problems
```java
// Anti-pattern: Many deletes
for (int i = 0; i < 1000000; i++) {
    session.execute("DELETE FROM users WHERE id = ?", i);
}
// Creates 1M tombstones â†’ slow reads!
```

**Impact**: Accumulated tombstones â†’ slow reads â†’ timeouts

**Best Practice**: Use TTL for auto-expiring data instead of explicit deletes

---

## ğŸ›¡ï¸ High Availability Features

### ğŸ¤ Hinted Handoff

When a node is **down**:
```
1. Coordinator receives write
2. Target node is unavailable
3. Coordinator stores "hint" on local disk
   (hint = data + target node info)
4. Every 10 minutes, checks if target recovered
5. Replays hints when target is back
```

**Limitations**:
- âš ï¸ Hints stored for **3 hours** (default)
- âš ï¸ Data lost if coordinator dies
- âš ï¸ Not a replacement for repair

### ğŸ‘¥ Gossip Protocol

**Purpose**: Node discovery and failure detection

Every node, **every second**:
```
1. Picks 1-3 random nodes
2. Exchanges state information
   - Endpoint state
   - Generation number
   - Heartbeat
   - Application state
3. Updates local view of cluster
```

#### Generation Number
- Incremented on **node restart**
- Helps detect node restarts vs. network issues

### ğŸ©º Failure Detection: Phi Accrual

**Traditional Heartbeat**: Boolean (alive/dead)
```
Problem: Hard to pick timeout
  - Too short â†’ false positives
  - Too long â†’ slow detection
```

**Phi Accrual Solution**: Suspicion level (0 to âˆ)
```
Î¦ (phi) value:
â”œâ”€â”€ 0-1: Healthy
â”œâ”€â”€ 1-5: Slightly suspicious
â”œâ”€â”€ 5-8: Moderately suspicious  
â””â”€â”€ 8+: Likely dead (default threshold = 8)
```

**Adaptive**: Uses **historical heartbeat data** to adjust threshold

---

## ğŸ•µï¸ Snitch (Topology Awareness)

**Purpose**: Understands cluster topology

**Responsibilities**:
1. ğŸ—ºï¸ Determines datacenter/rack of nodes
2. âš¡ Monitors read latencies
3. ğŸ“ Guides replica placement
4. ğŸš« Avoids slow nodes

**Common Snitches**:
- **SimpleSnitch**: Single datacenter
- **GossipingPropertyFileSnitch**: Multi-DC (production)
- **Ec2Snitch**: AWS deployments
- **GoogleCloudSnitch**: GCP deployments

---

## ğŸ’¾ SSTable Storage on Disk

Each SSTable consists of:

```
ğŸ“‚ SSTable Components
â”œâ”€â”€ ğŸ“„ Data File (actual data)
â”œâ”€â”€ ğŸ”‘ Partition Index (partition key â†’ offset)
â”œâ”€â”€ ğŸ“‹ Partition Summary (in RAM, subset of index)
â”œâ”€â”€ ğŸŒ¸ Bloom Filter (in RAM)
â”œâ”€â”€ ğŸ“Š Statistics (metadata)
â””â”€â”€ ğŸ—œï¸ Compression Info
```

---

## ğŸ“ Interview Key Points

### ğŸ”¥ Why Cassandra for Writes?
```
1. Append-only CommitLog (sequential I/O)
2. In-memory MemTable (RAM speed)
3. Batch writes to SSTables
4. No locks/latches needed
5. Linear scalability
```

### ğŸ“– Why Reads Can Be Slower?
```
1. Check MemTable
2. Check Row Cache
3. Scan multiple SSTables
4. Merge results by timestamp
5. Return latest version
```

**Solution**: Compaction + proper cache tuning

### âš–ï¸ CAP Theorem Position
```
Cassandra = AP System
â”œâ”€â”€ A: Availability (always responds)
â”œâ”€â”€ P: Partition Tolerance (works during network splits)
â””â”€â”€ C: Eventually Consistent (tunable)
```

**Tunable Consistency**: Can achieve CP by using QUORUM/ALL

### ğŸ¯ When to Use Cassandra?
âœ… **Good for**:
- High write throughput needs
- Linear scalability required
- Multi-datacenter deployment
- Time-series data
- No complex joins needed

âŒ **Bad for**:
- Complex queries/joins
- Strong ACID transactions
- Frequent updates/deletes
- Small datasets (<100GB)

---

## ğŸ› ï¸ Common Interview Questions

### Q1: How does Cassandra achieve high write performance?
**Answer**: 
- Sequential writes to CommitLog (append-only)
- In-memory MemTable for immediate acknowledgment
- No read-before-write
- Batch flushes to immutable SSTables
- No locking required

### Q2: Explain the read path in Cassandra
**Answer**:
1. Check Row Cache â†’ return if hit
2. Check Bloom filters of SSTables
3. Use Key Cache or Partition Index to find location
4. Read from MemTable + relevant SSTables
5. Merge results using timestamp (last write wins)
6. Return to client

### Q3: What are tombstones and why are they problematic?
**Answer**:
- Tombstones mark deleted data (soft delete)
- Needed for eventual consistency
- Problem: Accumulation slows reads (must scan all tombstones)
- Solution: Use TTL, tune gc_grace_seconds, monitor tombstone warnings

### Q4: Difference between Cassandra and MongoDB?
**Answer**:
```
Cassandra:
- Wide-column store
- AP (Available, Partition-tolerant)
- Peer-to-peer (no master)
- Better for writes
- Multi-DC built-in

MongoDB:
- Document store
- CP (Consistent, Partition-tolerant)
- Primary-Secondary (has master)
- Flexible schema
- Better for complex queries
```

### Q5: How to choose consistency level?
**Answer**:
```
Use Case â†’ Consistency Level
â”œâ”€â”€ Strong consistency â†’ QUORUM/ALL
â”œâ”€â”€ Fast reads â†’ ONE
â”œâ”€â”€ Fast writes â†’ ONE
â”œâ”€â”€ Multi-DC consistency â†’ LOCAL_QUORUM
â””â”€â”€ Critical data â†’ EACH_QUORUM
```

---

## ğŸ“š Quick Reference

### Consistency Formulas
```
Quorum = floor(RF/2) + 1

For RF=3:
- Quorum = 2
- Strong consistency: Read(QUORUM) + Write(QUORUM)
```

### Key Terminology Cheat Sheet
```
ğŸ”‘ Partition Key: Determines node placement
ğŸ”€ Clustering Key: Determines sort order within partition
ğŸ“Š Primary Key: Partition Key + Clustering Key(s)
ğŸ’ Token: Hash of partition key
ğŸ”„ Replica: Copy of data on different node
â±ï¸ Timestamp: Conflict resolution (last write wins)
ğŸª¦ Tombstone: Soft delete marker
ğŸŒ¸ Bloom Filter: Probabilistic existence check
```

---

## ğŸš€ Performance Tuning Tips

1. **Choose appropriate Compaction Strategy** for workload
2. **Tune cache sizes** based on RAM availability
3. **Use TTL** instead of deletes when possible
4. **Monitor tombstone warnings** in logs
5. **Partition data evenly** (avoid hot partitions)
6. **Use appropriate consistency levels** (don't always use QUORUM)
7. **Enable compression** for large datasets
8. **Run regular repairs** in multi-DC setups

---

## âœ… Best Practices

### Data Modeling
```java
// âœ… Good: Time-series partition
CREATE TABLE sensor_data (
    sensor_id UUID,
    day DATE,
    hour INT,
    reading DOUBLE,
    PRIMARY KEY ((sensor_id, day), hour)
);

// âŒ Bad: Unbounded partition
CREATE TABLE user_events (
    user_id UUID,
    event_time TIMESTAMP,
    event_data TEXT,
    PRIMARY KEY (user_id, event_time)
);
// Problem: One user's partition grows forever
```

### Write Patterns
```java
// âœ… Good: Batch same partition
BatchStatement batch = new BatchStatement();
batch.add(insert1); // same partition key
batch.add(insert2); // same partition key
session.execute(batch);

// âŒ Bad: Batch different partitions
// Coordinator becomes bottleneck
```

---

**Last Updated**: January 2026
**Version**: Cassandra 4.x+

---

*ğŸ’¡ Pro Tip: Always model your queries first, then design your tables. "Model your queries, not your data!"*