# ğŸ“Š Apache Cassandra - Wide Column NoSQL Database

## ğŸ“‘ Table of Contents


1. [Overview](#-overview)
2. [Why "Wide Column" Database?](#-why-wide-column-database)
3. [Data Model](#-data-model)
4. [Use Cases](#-use-cases)
5. [Cassandra as Time-Series Database](#-cassandra-as-time-series-database)
    - [Time-Series Data Modeling Patterns](#-time-series-data-modeling-patterns)
    - [Time-Series Best Practices](#-time-series-best-practices)
    - [Time-Series Query Patterns](#-time-series-query-patterns)
6. [Partitioning & Data Distribution](#-partitioning--data-distribution)
7. [Replication](#-replication)
8. [Write Path (High Performance Secret)](#-write-path-high-performance-secret)
9. [Read Path](#-read-path)
10. [Compaction](#-compaction)
11. [Tombstones (The Delete Challenge)](#-tombstones-the-delete-challenge)
12. [High Availability Features](#-high-availability-features)
13. [Snitch (Topology Awareness)](#-snitch-topology-awareness)
14. [SSTable Storage on Disk](#-sstable-storage-on-disk)
15. [Interview Key Points](#-interview-key-points)
16. [Common Interview Questions](#-common-interview-questions)
17. [Quick Reference](#-quick-reference)
18. [Performance Tuning Tips](#-performance-tuning-tips)
19. [Best Practices](#-best-practices)

---

## ğŸ¯ Overview

Apache Cassandra is a **distributed, highly available, eventually consistent, wide-column NoSQL database** designed to handle huge amounts of structured data across many commodity servers.

### âœ¨ Key Characteristics
- ğŸ”„ **Peer-to-peer architecture** - No single point of failure
- ğŸ“ˆ **Linear scalability** - Performance increases linearly with added nodes
- âœï¸ **Write-optimized** - Exceptional write throughput
- ğŸŒ **Multi-datacenter replication** - Geographic distribution support
- ğŸš« **No master node** - Every node can serve client requests

---

## ğŸ¤” Why "Wide Column" Database?

### Traditional Relational DB vs Cassandra

**Traditional Relational DB**:
```
User Table (Fixed Schema)
+----+-------+-------+-----+
| ID | Name  | Email | Age |
+----+-------+-------+-----+
| 1  | Alice | a@... | 25  |
| 2  | Bob   | b@... | 30  |
+----+-------+-------+-----+
All rows MUST have same columns
```

**Cassandra Wide Column**:
```
User Table (Flexible Schema)
+----+-------+-------+-----+---------+----------+
| ID | Name  | Email | Age | Country | Hobbies  |
+----+-------+-------+-----+---------+----------+
| 1  | Alice | a@... | 25  | USA     | Reading  |
+----+-------+-------+-----+---------+----------+
| 2  | Bob   | b@... |     |         |          |
+----+-------+-------+-----+---------+----------+
Different rows CAN have different columns!
```

**Key Insight**: Each row can have **millions of columns**!

### Real Example: User Activity Tracking
```java
// Table structure
CREATE TABLE user_activity (
    user_id UUID,
    activity_date DATE,
    // Clustering columns can be WIDE (millions possible)
    activity_timestamp TIMESTAMP,
    activity_type TEXT,
    activity_data TEXT,
    PRIMARY KEY ((user_id, activity_date), activity_timestamp)
);

// For user "Alice" on "2026-01-21":
Row 1: [user_id=Alice, date=2026-01-21]
  â”œâ”€â”€ timestamp=09:00:00 â†’ {type: "login", data: "mobile"}
  â”œâ”€â”€ timestamp=09:15:23 â†’ {type: "view", data: "product_123"}
  â”œâ”€â”€ timestamp=09:16:45 â†’ {type: "cart_add", data: "product_123"}
  â”œâ”€â”€ timestamp=09:20:12 â†’ {type: "checkout", data: "order_456"}
  â””â”€â”€ ... (potentially millions more columns/activities)
```

### Visualization
```
Partition Key: user_id=Alice, date=2026-01-21
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition (stored together on same nodes)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Column 1: ts=09:00:00 â†’ login                 â”‚
â”‚ Column 2: ts=09:15:23 â†’ view_product          â”‚
â”‚ Column 3: ts=09:16:45 â†’ add_to_cart           â”‚
â”‚ Column 4: ts=09:20:12 â†’ checkout              â”‚
â”‚ ... (can grow to millions of columns)         â”‚
â”‚ Column N: ts=23:59:59 â†’ logout                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘
     This is "WIDE"!
```

### Benefits of Wide Column Model
- ğŸš€ All related data in one partition (fast reads)
- ğŸ“Š Flexible schema (add columns dynamically)
- âš¡ No JOINs needed (denormalized)
- ğŸ’¾ Efficient storage of sparse data

---

## ğŸ—ï¸ Data Model

### Basic Building Blocks

```
ğŸ“¦ Column (Key-Value Pair)
   â”œâ”€â”€ Column Key: Unique identifier
   â””â”€â”€ Column Value: Stores one or collection of values

ğŸ“‹ Row: Collection of columns
ğŸ“ Table: Container of rows
ğŸ—„ï¸ Keyspace: Container for tables (spans one or more nodes)
ğŸŒ Cluster: Container of keyspaces
ğŸ’» Node: Computer system running Cassandra instance
```

### Hierarchy
```
Cluster
  â””â”€â”€ Keyspace(s)
       â””â”€â”€ Table(s)
            â””â”€â”€ Row(s)
                 â””â”€â”€ Column(s)
```

---

## ğŸ¯ Use Cases
Perfect for:
- â±ï¸ Time-series data (IoT sensor logs)
- ğŸ“Š Streaming services
- ğŸ’¬ Messaging applications
- ğŸ“ Activity logging
- ğŸŒ¡ï¸ Weather data
- ğŸ“ˆ Financial transactions

---

## â° Cassandra as Time-Series Database

### ğŸ¯ Why Cassandra Excels at Time-Series?

**Time-Series Characteristics**:
1. âœï¸ **Write-heavy** (continuous data ingestion)
2. ğŸ“… **Time-ordered** (sorted by timestamp)
3. ğŸ“Š **Immutable** (historical data rarely updated)
4. ğŸ—‘ï¸ **TTL-friendly** (old data auto-expires)

**Perfect match for Cassandra!**

### ğŸ—ï¸ Time-Series Data Modeling Patterns

#### Pattern 1: IoT Sensor Data
```java
// Schema Design
CREATE TABLE sensor_readings (
    sensor_id UUID,          // Which sensor
    bucket_date DATE,        // Partition by day (prevents unbounded growth)
    reading_time TIMESTAMP,  // Clustering key (sorts data)
    temperature DOUBLE,
    humidity DOUBLE,
    pressure DOUBLE,
    PRIMARY KEY ((sensor_id, bucket_date), reading_time)
) WITH CLUSTERING ORDER BY (reading_time DESC)  // Newest first
  AND compaction = {
    'class': 'TimeWindowCompactionStrategy',
    'compaction_window_unit': 'DAYS',
    'compaction_window_size': '1'
  }
  AND default_time_to_live = 2592000;  // 30 days TTL

// Insert Example
PreparedStatement prepared = session.prepare(
    "INSERT INTO sensor_readings (sensor_id, bucket_date, reading_time, temperature, humidity, pressure) " +
    "VALUES (?, ?, ?, ?, ?, ?) USING TTL ?"
);

BoundStatement bound = prepared.bind(
    sensorId,                           // UUID
    LocalDate.now(),                    // bucket_date
    Instant.now(),                      // reading_time
    23.5,                               // temperature
    65.0,                               // humidity
    1013.25,                            // pressure
    2592000                             // 30 days in seconds
);
session.execute(bound);

// Query: Last 24 hours for sensor
ResultSet results = session.execute(
    "SELECT * FROM sensor_readings " +
    "WHERE sensor_id = ? AND bucket_date IN (?, ?) " +
    "AND reading_time > ? AND reading_time < ?",
    sensorId,
    LocalDate.now().minusDays(1),
    LocalDate.now(),
    Instant.now().minus(24, ChronoUnit.HOURS),
    Instant.now()
);
```

**ğŸ“Š Data Layout**:
```
Partition 1: sensor_123, 2026-01-21
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 23:59:59 â†’ {temp: 22.5, humidity: 60}       â”‚
â”‚ 23:59:58 â†’ {temp: 22.5, humidity: 60}       â”‚
â”‚ 23:59:57 â†’ {temp: 22.6, humidity: 61}       â”‚
â”‚ ... (86,400 readings per day max)            â”‚
â”‚ 00:00:01 â†’ {temp: 20.1, humidity: 55}       â”‚
â”‚ 00:00:00 â†’ {temp: 20.0, humidity: 55}       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
        Single partition read = FAST!
```

#### Pattern 2: Application Metrics
```java
// Bucketing by hour for high-frequency metrics
CREATE TABLE app_metrics (
    app_name TEXT,
    metric_name TEXT,
    hour_bucket TIMESTAMP,    // Round to hour: 2026-01-21 10:00:00
    metric_time TIMESTAMP,
    metric_value DOUBLE,
    PRIMARY KEY ((app_name, metric_name, hour_bucket), metric_time)
) WITH CLUSTERING ORDER BY (metric_time DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy'};

// Insert Example
public void recordMetric(String appName, String metricName, double value) {
    Instant now = Instant.now();
    Instant hourBucket = now.truncatedTo(ChronoUnit.HOURS);
    
    session.execute(
        "INSERT INTO app_metrics (app_name, metric_name, hour_bucket, metric_time, metric_value) " +
        "VALUES (?, ?, ?, ?, ?)",
        appName,
        metricName,
        hourBucket,
        now,
        value
    );
}

// Query: Get last hour's CPU metrics
public List<MetricReading> getLastHourMetrics(String appName) {
    Instant now = Instant.now();
    Instant hourBucket = now.truncatedTo(ChronoUnit.HOURS);
    
    ResultSet rs = session.execute(
        "SELECT metric_time, metric_value FROM app_metrics " +
        "WHERE app_name = ? AND metric_name = ? AND hour_bucket = ?",
        appName,
        "cpu_usage",
        hourBucket
    );
    
    List<MetricReading> readings = new ArrayList<>();
    for (Row row : rs) {
        readings.add(new MetricReading(
            row.getInstant("metric_time"),
            row.getDouble("metric_value")
        ));
    }
    return readings;
}
```

#### Pattern 3: Stock Prices (Financial Time-Series)
```java
CREATE TABLE stock_prices (
    symbol TEXT,              // AAPL, GOOGL, etc.
    trade_date DATE,          // Partition by day
    trade_time TIMESTAMP,
    price DECIMAL,
    volume BIGINT,
    PRIMARY KEY ((symbol, trade_date), trade_time)
) WITH CLUSTERING ORDER BY (trade_time ASC);  // Oldest first for stock data

// Materialized View for latest price (optional)
CREATE MATERIALIZED VIEW latest_stock_price AS
    SELECT symbol, trade_date, trade_time, price, volume
    FROM stock_prices
    WHERE symbol IS NOT NULL 
      AND trade_date IS NOT NULL 
      AND trade_time IS NOT NULL
    PRIMARY KEY (symbol, trade_date, trade_time)
    WITH CLUSTERING ORDER BY (trade_date DESC, trade_time DESC);

// Insert trade
public void recordTrade(String symbol, BigDecimal price, long volume) {
    session.execute(
        "INSERT INTO stock_prices (symbol, trade_date, trade_time, price, volume) " +
        "VALUES (?, ?, ?, ?, ?)",
        symbol,
        LocalDate.now(),
        Instant.now(),
        price,
        volume
    );
}

// Query: Day's trading history
public List<Trade> getDayTrades(String symbol, LocalDate date) {
    ResultSet rs = session.execute(
        "SELECT trade_time, price, volume FROM stock_prices " +
        "WHERE symbol = ? AND trade_date = ?",
        symbol,
        date
    );
    
    return StreamSupport.stream(rs.spliterator(), false)
        .map(row -> new Trade(
            row.getInstant("trade_time"),
            row.getBigDecimal("price"),
            row.getLong("volume")
        ))
        .collect(Collectors.toList());
}
```

### ğŸ”‘ Time-Series Best Practices

#### 1. **Partition Bucketing** (Critical!)
```java
// âŒ BAD: Unbounded partition
CREATE TABLE bad_timeseries (
    sensor_id UUID,
    reading_time TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY (sensor_id, reading_time)
);
// Problem: Single partition grows forever â†’ slow reads, hotspots

// âœ… GOOD: Bounded partitions
CREATE TABLE good_timeseries (
    sensor_id UUID,
    day DATE,              // or hour, week, month
    reading_time TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY ((sensor_id, day), reading_time)
);
// Benefit: Each partition has predictable size
```

**Bucketing Strategy**:
```
Data Rate         â†’ Bucket Size
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
1/second          â†’ Hour or Day
10/second         â†’ Hour
100/second        â†’ Hour (consider smaller)
1000+/second      â†’ Minute or custom
```

#### 2. **Use Time Window Compaction Strategy**
```java
ALTER TABLE sensor_readings 
WITH compaction = {
    'class': 'TimeWindowCompactionStrategy',
    'compaction_window_unit': 'DAYS',
    'compaction_window_size': '1',
    'timestamp_resolution': 'MICROSECONDS'
};
```

**Why TWCS?**
- âœ… Time-series data rarely updated
- âœ… SSTables organized by time window
- âœ… Expired data easily dropped (no merge needed)
- âœ… Better than STCS/LCS for time-series

#### 3. **Leverage TTL (Auto-Expiration)**
```java
// Table-level default TTL
CREATE TABLE sensor_data (...) 
WITH default_time_to_live = 604800;  // 7 days

// Row-level TTL (overrides table default)
INSERT INTO sensor_data (...) 
VALUES (...) 
USING TTL 2592000;  // 30 days for this row

// No TTL for specific row (keep forever)
INSERT INTO sensor_data (...) 
VALUES (...) 
USING TTL 0;
```

#### 4. **Batching Writes**
```java
// âœ… GOOD: Batch writes to SAME partition
BatchStatement batch = new BatchStatement(BatchStatement.Type.UNLOGGED);

for (SensorReading reading : readings) {
    batch.add(prepared.bind(
        reading.getSensorId(),
        reading.getDate(),
        reading.getTime(),
        reading.getValue()
    ));
}
session.execute(batch);

// âŒ BAD: Don't batch across partitions (coordinator bottleneck)
```

#### 5. **Downsampling for Long-Term Storage**
```java
// Raw data: Keep for 7 days
CREATE TABLE sensor_readings_raw (
    sensor_id UUID,
    hour_bucket TIMESTAMP,
    reading_time TIMESTAMP,
    value DOUBLE,
    PRIMARY KEY ((sensor_id, hour_bucket), reading_time)
) WITH default_time_to_live = 604800;  // 7 days

// Aggregated data: Keep for 1 year
CREATE TABLE sensor_readings_hourly (
    sensor_id UUID,
    day DATE,
    hour INT,
    avg_value DOUBLE,
    min_value DOUBLE,
    max_value DOUBLE,
    count BIGINT,
    PRIMARY KEY ((sensor_id, day), hour)
) WITH default_time_to_live = 31536000;  // 1 year

// Background job to downsample
public void downsampleToHourly(UUID sensorId, Instant hourStart) {
    // Read raw data for hour
    ResultSet rs = session.execute(
        "SELECT value FROM sensor_readings_raw " +
        "WHERE sensor_id = ? AND hour_bucket = ?",
        sensorId, hourStart
    );
    
    // Calculate aggregates
    DoubleSummaryStatistics stats = StreamSupport
        .stream(rs.spliterator(), false)
        .mapToDouble(row -> row.getDouble("value"))
        .summaryStatistics();
    
    // Write aggregated data
    session.execute(
        "INSERT INTO sensor_readings_hourly " +
        "(sensor_id, day, hour, avg_value, min_value, max_value, count) " +
        "VALUES (?, ?, ?, ?, ?, ?, ?)",
        sensorId,
        LocalDate.from(hourStart),
        hourStart.get(ChronoField.HOUR_OF_DAY),
        stats.getAverage(),
        stats.getMin(),
        stats.getMax(),
        stats.getCount()
    );
}
```

### ğŸ“Š Time-Series Query Patterns

#### Range Queries
```java
// Query: Last 24 hours
public List<Reading> getLast24Hours(UUID sensorId) {
    Instant now = Instant.now();
    LocalDate today = LocalDate.now();
    LocalDate yesterday = today.minusDays(1);
    
    ResultSet rs = session.execute(
        "SELECT reading_time, temperature, humidity " +
        "FROM sensor_readings " +
        "WHERE sensor_id = ? AND bucket_date IN (?, ?) " +
        "AND reading_time > ?",
        sensorId,
        yesterday,
        today,
        now.minus(24, ChronoUnit.HOURS)
    );
    
    return convertToReadings(rs);
}
```

#### Aggregations (Application-Side)
```java
// Cassandra doesn't have built-in aggregations for time-series
// Calculate in application or use Spark
public Map<String, Double> calculateHourlyAverages(UUID sensorId, LocalDate date) {
    ResultSet rs = session.execute(
        "SELECT reading_time, temperature FROM sensor_readings " +
        "WHERE sensor_id = ? AND bucket_date = ?",
        sensorId, date
    );
    
    return StreamSupport.stream(rs.spliterator(), false)
        .collect(Collectors.groupingBy(
            row -> row.getInstant("reading_time")
                     .truncatedTo(ChronoUnit.HOURS)
                     .toString(),
            Collectors.averagingDouble(row -> row.getDouble("temperature"))
        ));
}
```

### ğŸš€ Performance Tips for Time-Series

1. **Keep Partitions Bounded**
   ```
   Rule of thumb: < 100MB per partition
   Monitor: nodetool tablehistograms
   ```

2. **Use Appropriate Compaction**
   ```
   Time-series â†’ TWCS
   Time-series + updates â†’ LCS
   ```

3. **Set Proper TTL**
   ```
   Automatic cleanup, no tombstone accumulation
   ```

4. **Monitor Metrics**
   ```bash
   # Check partition sizes
   nodetool tablehistograms keyspace.table
   
   # Check SSTable count
   nodetool tablestats keyspace.table
   ```

5. **Consider Data Retention Policy**
   ```
   Hot data:  Raw, 7-30 days
   Warm data: Aggregated hourly, 90 days
   Cold data: Aggregated daily, 1 year+
   ```

### ğŸ¯ Real-World Time-Series Example

#### Netflix Viewing Analytics
```java
// Track user viewing sessions
CREATE TABLE viewing_sessions (
    user_id UUID,
    session_date DATE,
    session_start TIMESTAMP,
    content_id UUID,
    duration_seconds INT,
    playback_position INT,
    device_type TEXT,
    PRIMARY KEY ((user_id, session_date), session_start)
) WITH CLUSTERING ORDER BY (session_start DESC)
  AND compaction = {'class': 'TimeWindowCompactionStrategy'}
  AND default_time_to_live = 7776000;  // 90 days

// Insert viewing event
public void trackViewingSession(UUID userId, UUID contentId, 
                                 int duration, String device) {
    session.execute(
        "INSERT INTO viewing_sessions " +
        "(user_id, session_date, session_start, content_id, " +
        "duration_seconds, device_type) " +
        "VALUES (?, ?, ?, ?, ?, ?)",
        userId,
        LocalDate.now(),
        Instant.now(),
        contentId,
        duration,
        device
    );
}

// Query: User's recent viewing history
public List<ViewingSession> getRecentHistory(UUID userId, int days) {
    List<LocalDate> dates = IntStream.range(0, days)
        .mapToObj(i -> LocalDate.now().minusDays(i))
        .collect(Collectors.toList());
    
    ResultSet rs = session.execute(
        "SELECT session_start, content_id, duration_seconds, device_type " +
        "FROM viewing_sessions " +
        "WHERE user_id = ? AND session_date IN ?",
        userId,
        dates
    );
    
    return convertToSessions(rs);
}
```

### ğŸ“‹ Time-Series Checklist

- âœ… Partition by time bucket (hour/day/week)
- âœ… Use TimeWindowCompactionStrategy
- âœ… Set appropriate TTL
- âœ… Clustering key = timestamp
- âœ… Avoid unbounded partitions
- âœ… Consider downsampling for old data
- âœ… Monitor partition sizes
- âœ… Batch writes to same partition
- âœ… Use prepared statements
- âœ… Plan retention policy upfront

---

## ğŸ”‘ Partitioning & Data Distribution

### Partitioner

The **Partitioner** determines how data is distributed across the cluster using a **consistent hash ring**.

```
Request â†’ Partition Key â†’ Murmur3 Hash â†’ Token Ring â†’ Node Assignment
```

#### ğŸ” How Coordinator Finds Nodes

When a request arrives:
1. **Takes the partition key**
2. **Hashes it using Murmur3** (default hashing function)
3. **Maps hash to token ring**
4. **Builds preference list** (replica locations)

**Key Benefit**: Decouples client access from data ownership â†’ enables linear scalability without masters

---

## ğŸ” Replication

### Replication Factor (RF)
- **RF = 3** means each row stored on **3 different nodes**
- Each keyspace can have **different RF**

### ğŸ“‹ Replication Strategies

#### 1ï¸âƒ£ Simple Strategy
- For **single datacenter** deployments
- First replica placed by partitioner
- Subsequent replicas on **next nodes clockwise**

#### 2ï¸âƒ£ Network Topology Strategy (Production Default)
- For **multi-datacenter** deployments
- Different RF per datacenter
- Rack-aware placement

### ğŸ¯ Consistency Levels

```
Quorum = floor(RF/2 + 1)

Examples (RF = 3):
â”œâ”€â”€ ONE: 1 node must respond
â”œâ”€â”€ QUORUM: 2 nodes must respond  
â”œâ”€â”€ ALL: 3 nodes must respond
â”œâ”€â”€ LOCAL_QUORUM: Quorum in same DC
â””â”€â”€ EACH_QUORUM: Quorum in each DC
```

**Trade-off**: Higher consistency = Higher latency

---

## ğŸ“ Write Path (High Performance Secret)

### Write Flow
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Write Requestâ”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ CommitLog   â”‚    â”‚MemTable  â”‚
â”‚  (Disk)     â”‚    â”‚ (Memory) â”‚
â”‚  WAL/Append â”‚    â”‚  Sorted  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                        â”‚ (when full)
                        â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ SSTable  â”‚
                  â”‚  (Disk)  â”‚
                  â”‚Immutable â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
![Cassandra's write path](../images/cassandraWritePath.png)

### ğŸ”¥ Why Writes Are Fast
1. **Sequential append** to CommitLog (no seek time)
2. **In-memory writes** to MemTable (fast)
3. **Batch flushes** to SSTables (efficient)
4. **No read-before-write** (unlike B-trees)

### âš™ï¸ Components

#### CommitLog (Write-Ahead Log)
- ğŸ’¾ Durability guarantee
- ğŸ“Š Sequential writes (append-only)
- ğŸ”„ Replayed on node restart
- ğŸ“¦ **Segmented**: Multiple files, archived when data flushed

**Default**: 3 segments, each grows until threshold

#### MemTable
- ğŸ§  One per table per node (in-memory)
- ğŸ”€ Stores data **sorted by partition + clustering keys**
- âš¡ Serves reads for unflushed data
- ğŸ’§ Flushed when size threshold reached
![Storing data to commitlog and memtable](../images/storingDataToCommitLogAndMemTable.png)

**CommitLog vs MemTable**:
```
CommitLog: Sequential Order â†’ Fast writes
MemTable:  Sorted Order â†’ Fast reads
```

#### SSTable (Sorted String Table)
- ğŸ’½ Immutable on-disk files
- ğŸ”’ **Cannot be modified** after creation
- ğŸ—‘ï¸ Updates/Deletes = New write operations
- ğŸ“‘ Multiple SSTables per table

![Cassandra read operation workflow](../images/cassandraReadOperationWorkflow.png)

**Q: How to update/delete if immutable?**
**A**: Cassandra writes a new version with timestamp/tombstone

---

## ğŸ“– Read Path

### Read Flow

```
Read Request
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Row Cache  â”‚ (Hot rows)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ miss
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Key Cache  â”‚ (Partition key â†’ SSTable offset)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ miss
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Bloom Filter   â”‚ (Key exists in SSTable?)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚ probably yes
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition Index  â”‚
â”‚  Summary (RAM)   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Partition Index  â”‚
â”‚   File (Disk)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Data File  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![Anatomy of Cassandra write path](../images/anatomyOfCassandraWritePath.png)

### ğŸš€ Caching Layers

1. **Row Cache** ğŸ”¥
   - Caches entire frequently-read rows
   - Stored off-heap
   - Best for read-heavy workloads

2. **Key Cache** ğŸ—ï¸
   - Maps partition keys â†’ SSTable offsets
   - Stored on-heap
   - Updated on every write (can slow writes)

3. **Chunk Cache** ğŸ“¦
   - Uncompressed data chunks from SSTables
   - Frequently accessed data

![Anatomy of Cassandra read path](../images/anatomyOfCassandraReadPath.png)

### ğŸŒ¸ Bloom Filters
- **Probabilistic data structure**
- Tells if key **might exist** in SSTable
- **No false negatives** (if says "no", definitely not there)
- **Possible false positives** (if says "yes", might not be there)
- One per SSTable (stored in RAM)

**Benefit**: Avoids unnecessary disk reads

---

## ğŸ”§ Compaction

### ğŸ’¡ Why Compaction?

```
Problem: Multiple SSTables accumulate over time
         â†“
Solution: Merge them into fewer, larger SSTables
         â†“
Benefits: 
  - Faster reads (fewer files to check)
  - Remove deleted data (tombstones)
  - Reclaim disk space
```

### ğŸ“Š Compaction Strategies

#### 1ï¸âƒ£ Size-Tiered Compaction Strategy (STCS)
- Merges SSTables of **similar size**
- **Best for**: Write-heavy, time-series data
- **Drawback**: Can use 2x disk space temporarily

#### 2ï¸âƒ£ Leveled Compaction Strategy (LCS)
- Organizes SSTables into **levels** (L0, L1, L2...)
- Each level 10x larger than previous
- **Best for**: Read-heavy workloads
- **Benefit**: Predictable read performance

#### 3ï¸âƒ£ Time-Window Compaction Strategy (TWCS)
- Compacts within **time windows**
- **Best for**: Time-series with TTL
- **Example**: Compact hourly/daily buckets

---

## ğŸª¦ Tombstones (The Delete Challenge)

### ğŸ¤” The Problem
```
Node A: DELETE key=123
Node B: Offline (missed the delete)
         â†“
Node B comes back online
         â†“
Repair runs â†’ Node B resurrects deleted data! ğŸ˜±
```

### âœ… The Solution: Tombstones
- Delete = **Soft delete** (mark with tombstone)
- Tombstone = marker with **timestamp**
- Default TTL: **10 days** (gc_grace_seconds)
- Removed during compaction

### âš ï¸ Tombstone Problems
```java
// Anti-pattern: Many deletes
for (int i = 0; i < 1000000; i++) {
    session.execute("DELETE FROM users WHERE id = ?", i);
}
// Creates 1M tombstones â†’ slow reads!
```

**Impact**: Accumulated tombstones â†’ slow reads â†’ timeouts

**Best Practice**: Use TTL for auto-expiring data instead of explicit deletes

---

## ğŸ›¡ï¸ High Availability Features

### ğŸ¤ Hinted Handoff

When a node is **down**:
```
1. Coordinator receives write
2. Target node is unavailable
3. Coordinator stores "hint" on local disk
   (hint = data + target node info)
4. Every 10 minutes, checks if target recovered
5. Replays hints when target is back
```

**Limitations**:
- âš ï¸ Hints stored for **3 hours** (default)
- âš ï¸ Data lost if coordinator dies
- âš ï¸ Not a replacement for repair

### ğŸ‘¥ Gossip Protocol

**Purpose**: Node discovery and failure detection

Every node, **every second**:
```
1. Picks 1-3 random nodes
2. Exchanges state information
   - Endpoint state
   - Generation number
   - Heartbeat
   - Application state
3. Updates local view of cluster
```

#### Generation Number
- Incremented on **node restart**
- Helps detect node restarts vs. network issues

### ğŸ©º Failure Detection: Phi Accrual

**Traditional Heartbeat**: Boolean (alive/dead)
```
Problem: Hard to pick timeout
  - Too short â†’ false positives
  - Too long â†’ slow detection
```

**Phi Accrual Solution**: Suspicion level (0 to âˆ)
```
Î¦ (phi) value:
â”œâ”€â”€ 0-1: Healthy
â”œâ”€â”€ 1-5: Slightly suspicious
â”œâ”€â”€ 5-8: Moderately suspicious  
â””â”€â”€ 8+: Likely dead (default threshold = 8)
```

**Adaptive**: Uses **historical heartbeat data** to adjust threshold

---

## ğŸ•µï¸ Snitch (Topology Awareness)

**Purpose**: Understands cluster topology

**Responsibilities**:
1. ğŸ—ºï¸ Determines datacenter/rack of nodes
2. âš¡ Monitors read latencies
3. ğŸ“ Guides replica placement
4. ğŸš« Avoids slow nodes

**Common Snitches**:
- **SimpleSnitch**: Single datacenter
- **GossipingPropertyFileSnitch**: Multi-DC (production)
- **Ec2Snitch**: AWS deployments
- **GoogleCloudSnitch**: GCP deployments

---

## ğŸ’¾ SSTable Storage on Disk

Each SSTable consists of:

```
ğŸ“‚ SSTable Components
â”œâ”€â”€ ğŸ“„ Data File (actual data)
â”œâ”€â”€ ğŸ”‘ Partition Index (partition key â†’ offset)
â”œâ”€â”€ ğŸ“‹ Partition Summary (in RAM, subset of index)
â”œâ”€â”€ ğŸŒ¸ Bloom Filter (in RAM)
â”œâ”€â”€ ğŸ“Š Statistics (metadata)
â””â”€â”€ ğŸ—œï¸ Compression Info
```

---

## ğŸ“ Interview Key Points

### ğŸ”¥ Why Cassandra for Writes?
```
1. Append-only CommitLog (sequential I/O)
2. In-memory MemTable (RAM speed)
3. Batch writes to SSTables
4. No locks/latches needed
5. Linear scalability
```

### ğŸ“– Why Reads Can Be Slower?
```
1. Check MemTable
2. Check Row Cache
3. Scan multiple SSTables
4. Merge results by timestamp
5. Return latest version
```

**Solution**: Compaction + proper cache tuning

### âš–ï¸ CAP Theorem Position
```
Cassandra = AP System
â”œâ”€â”€ A: Availability (always responds)
â”œâ”€â”€ P: Partition Tolerance (works during network splits)
â””â”€â”€ C: Eventually Consistent (tunable)
```

**Tunable Consistency**: Can achieve CP by using QUORUM/ALL

### ğŸ¯ When to Use Cassandra?
âœ… **Good for**:
- High write throughput needs
- Linear scalability required
- Multi-datacenter deployment
- Time-series data
- No complex joins needed

âŒ **Bad for**:
- Complex queries/joins
- Strong ACID transactions
- Frequent updates/deletes
- Small datasets (<100GB)

---

## ğŸ› ï¸ Common Interview Questions

### Q1: How does Cassandra achieve high write performance?
**Answer**: 
- Sequential writes to CommitLog (append-only)
- In-memory MemTable for immediate acknowledgment
- No read-before-write
- Batch flushes to immutable SSTables
- No locking required

### Q2: Explain the read path in Cassandra
**Answer**:
1. Check Row Cache â†’ return if hit
2. Check Bloom filters of SSTables
3. Use Key Cache or Partition Index to find location
4. Read from MemTable + relevant SSTables
5. Merge results using timestamp (last write wins)
6. Return to client

### Q3: What are tombstones and why are they problematic?
**Answer**:
- Tombstones mark deleted data (soft delete)
- Needed for eventual consistency
- Problem: Accumulation slows reads (must scan all tombstones)
- Solution: Use TTL, tune gc_grace_seconds, monitor tombstone warnings

### Q4: Difference between Cassandra and MongoDB?
**Answer**:
```
Cassandra:
- Wide-column store
- AP (Available, Partition-tolerant)
- Peer-to-peer (no master)
- Better for writes
- Multi-DC built-in

MongoDB:
- Document store
- CP (Consistent, Partition-tolerant)
- Primary-Secondary (has master)
- Flexible schema
- Better for complex queries
```

### Q5: How to choose consistency level?
**Answer**:
```
Use Case â†’ Consistency Level
â”œâ”€â”€ Strong consistency â†’ QUORUM/ALL
â”œâ”€â”€ Fast reads â†’ ONE
â”œâ”€â”€ Fast writes â†’ ONE
â”œâ”€â”€ Multi-DC consistency â†’ LOCAL_QUORUM
â””â”€â”€ Critical data â†’ EACH_QUORUM
```

### Q6: Why is Cassandra called a "Wide Column" database?
**Answer**:
- Each row can have different columns (flexible schema)
- Single partition can contain millions of columns (wide)
- Columns are not fixed across rows
- Example: User activity partition can have millions of timestamp-based columns
- "Wide" refers to the unlimited columns per partition, not table width

### Q7: How to use Cassandra for time-series data?
**Answer**:
Key strategies:
- **Partition by time buckets** (hour/day) to prevent unbounded growth
- Use **TimeWindowCompactionStrategy** for efficient old data removal
- Implement **TTL** for automatic data expiration
- **Clustering by timestamp** for sorted reads
- Example: `PRIMARY KEY ((sensor_id, day), timestamp)`
- Downsample old data for long-term storage

---

## ğŸ“š Quick Reference

### Consistency Formulas
```
Quorum = floor(RF/2) + 1

For RF=3:
- Quorum = 2
- Strong consistency: Read(QUORUM) + Write(QUORUM)
```

### Key Terminology Cheat Sheet
```
ğŸ”‘ Partition Key: Determines node placement
ğŸ”€ Clustering Key: Determines sort order within partition
ğŸ“Š Primary Key: Partition Key + Clustering Key(s)
ğŸ’ Token: Hash of partition key
ğŸ”„ Replica: Copy of data on different node
â±ï¸ Timestamp: Conflict resolution (last write wins)
ğŸª¦ Tombstone: Soft delete marker
ğŸŒ¸ Bloom Filter: Probabilistic existence check
```

---

## ğŸš€ Performance Tuning Tips

1. **Choose appropriate Compaction Strategy** for workload
2. **Tune cache sizes** based on RAM availability
3. **Use TTL** instead of deletes when possible
4. **Monitor tombstone warnings** in logs
5. **Partition data evenly** (avoid hot partitions)
6. **Use appropriate consistency levels** (don't always use QUORUM)
7. **Enable compression** for large datasets
8. **Run regular repairs** in multi-DC setups

---

## âœ… Best Practices

### Data Modeling
```java
// âœ… Good: Time-series partition
CREATE TABLE sensor_data (
    sensor_id UUID,
    day DATE,
    hour INT,
    reading DOUBLE,
    PRIMARY KEY ((sensor_id, day), hour)
);

// âŒ Bad: Unbounded partition
CREATE TABLE user_events (
    user_id UUID,
    event_time TIMESTAMP,
    event_data TEXT,
    PRIMARY KEY (user_id, event_time)
);
// Problem: One user's partition grows forever
```

### Write Patterns
```java
// âœ… Good: Batch same partition
BatchStatement batch = new BatchStatement();
batch.add(insert1); // same partition key
batch.add(insert2); // same partition key
session.execute(batch);

// âŒ Bad: Batch different partitions
// Coordinator becomes bottleneck
```

---

**Last Updated**: January 2026
**Version**: Cassandra 4.x+

---

*ğŸ’¡ Pro Tip: Always model your queries first, then design your tables. "Model your queries, not your data!"*